URL <- 'http://www.bbc.com/sport/winter-olympics/2014/medals/countries'
# Get and parse all tables on the webpage
tables <- URL %>% GET() %>%
content(as = 'parsed') %>%
readHTMLTable()
names(tables)
ExpensesTable <- tables[[1]]
head(ExpensesTable)[, 1:2]
View(ExpensesTable)
View(ExpensesTable)
?head
library(httr)
library(dplyr)
library(XML)
URL <- 'http://www.bbc.com/sport/winter-olympics/2014/medals/countries'
# Get and parse all tables on the webpage
tables <- URL %>% GET() %>%
content(as = 'parsed') %>%
readHTMLTable()
names(tables)
ExpensesTable <- tables[[1]]
head(ExpensesTable)[, 1:3]
View(ExpensesTable)
View(ExpensesTable)
head(ExpensesTable)[, 1:7]
names(ExpensesTable)[1]<-paste("ID")
names(ExpensesTable)[2]<-paste("Country")
names(ExpensesTable)[3]<-paste("Gold")
names(ExpensesTable)[4]<-paste("Silver")
names(ExpensesTable)[5]<-paste("Bronce")
names(ExpensesTable)[6]<-paste("Total")
View(ExpensesTable)
View(ExpensesTable)
descending <- arrange(Total, Gold, Silver, Bronce)
head(descending)[1:7, ]
descending <- arrange(Total, desc(Gold))
head(descending)[1:7, ]
group_total<- group_by(Total, Gold, Silver, Bronce)
head(group_data)[1:5, ]
group_total<- group_by(Total, Gold, Silver, Bronce)
head(group_total)[1:5, ]
group_total<- group_by(Total)
group_total<- group_by(ExpensesTable, Total, Gold, Silver, Bronce)
head(group_total)[1:5, ]
descending <- arrange(ExpensesTable, desc(Total))
head(descending)[1:7, ]
head(descending)[1:20, ]
View(descending)
View(descending)
View(ExpensesTable)
View(ExpensesTable)
View(descending)
View(descending)
?clas
?class
class(ExpensesTable)
View(descending)
class(descending)
class(ExpensesTable$)
class(ExpensesTable$Total)
?as.numeric
as.numeric(ExpensesTable$Total)
class(ExpensesTable$Total)
as.numeric(descending <- arrange(ExpensesTable, desc(Total)))
as.numeric(head(descending)[1:5, ])
descending <- as.numeric(arrange(ExpensesTable, desc(Total)))
library(httr)
library(dplyr)
library(XML)
URL <- 'http://www.bbc.com/sport/winter-olympics/2014/medals/countries'
# Get and parse all tables on the webpage
all_tables <- URL %>% GET() %>%
content(as = 'parsed') %>%
readHTMLTable()
names(all_tables) #Table has no ID
##Convert to a data frame
medals <- as.data.frame(all_tables)
## Clean###
#Drop unwanted variables
medals <- select(medals, -Null.V1, -NULL.V7)
#Give new Variables names
names(medals) <- c('Country', 'Gold', 'Silver', 'Bronze', 'Total')
#COnvert variables class
medals$Country<-as.character(medals$Country)
for(i in 2:5) medals[, i]<-as.integer(medals[,i])
#Sort by total medals in descending order
medals <- arrange(medals, desc(Total))
library(httr)
library(dplyr)
library(XML)
URL <- 'http://www.bbc.com/sport/winter-olympics/2014/medals/countries'
# Get and parse all tables on the webpage
all_tables <- URL %>% GET() %>%
content(as = 'parsed') %>%
readHTMLTable()
names(all_tables) #Table has no ID
##Convert to a data frame
medals <- as.data.frame(all_tables)
## Clean###
#Drop unwanted variables
medals <- select(medals, -Null.V1, -NULL.V7)
#Give new Variables names
names(medals) <- c('Country', 'Gold', 'Silver', 'Bronze', 'Total')
#COnvert variables class
medals$Country<-as.character(medals$Country)
for(i in 2:5) medals[, i]<-as.integer(medals[,i])
#Sort by total medals in descending order
medals <- arrange(medals, desc(Total))
View(medals)
View(medals)
library(httr)
library(dplyr)
library(XML)
URL <- 'http://www.bbc.com/sport/winter-olympics/2014/medals/countries'
# Get and parse all tables on the webpage
all_tables <- URL %>% GET() %>%
content(as = 'parsed') %>%
readHTMLTable()
names(all_tables) #Table has no ID
##Convert to a data frame
medals <- as.data.frame(all_tables)
## Clean###
#Drop unwanted variables
medals <- select(medals, -Null.V1, -NULL.V7)
#Give new Variables names
names(medals) <- c('ID', 'Country', 'Gold', 'Silver', 'Bronze', 'Total')
#COnvert variables class
medals$Country<-as.character(medals$Country)
for(i in 2:5) medals[, i]<-as.integer(medals[,i])
#Sort by total medals in descending order
medals <- arrange(medals, desc(Total))
View(medals)
View(medals)
View(medals)
library(httr)
library(dplyr)
library(XML)
URL <- 'http://www.bbc.com/sport/winter-olympics/2014/medals/countries'
# Get and parse all tables on the webpage
all_tables <- URL %>% GET() %>%
content(as = 'parsed') %>%
readHTMLTable()
names(all_tables) #Table has no ID
##Convert to a data frame
medals <- as.data.frame(all_tables)
## Clean###
#Drop unwanted variables
medals <- select(medals, -Null.V1, -NULL.V7)
#Give new Variables names
names(medals) <- c('ID', 'Country', 'Gold', 'Silver', 'Bronze', 'Total', )
#Convert variables class
medals$Country<-as.character(medals$Country)
for(i in 3:5) medals[, i]<-as.integer(medals[,i])
#Sort by total medals in descending order
medals <- arrange(medals, desc(Total))
library(httr)
library(dplyr)
library(XML)
URL <- 'http://www.bbc.com/sport/winter-olympics/2014/medals/countries'
# Get and parse all tables on the webpage
all_tables <- URL %>% GET() %>%
content(as = 'parsed') %>%
readHTMLTable()
names(all_tables) #Table has no ID
##Convert to a data frame
medals <- as.data.frame(all_tables)
View(medals)
## Clean###
#Drop unwanted variables
medals <- select(medals, Null.V1, NULL.V7)
View(medals)
View(medals)
## Clean###
#Drop unwanted variables
medals <- select(medals, NULL.V1, NULL.V7)
#Give new Variables names
names(medals) <- c('Country', 'Gold', 'Silver', 'Bronze', 'Total')
View(medals)
View(medals)
View(medals)
library(httr)
library(dplyr)
library(XML)
URL <- 'http://www.bbc.com/sport/winter-olympics/2014/medals/countries'
# Get and parse all tables on the webpage
all_tables <- URL %>% GET() %>%
content(as = 'parsed') %>%
readHTMLTable()
names(all_tables) #Table has no ID
##Convert to a data frame
medals <- as.data.frame(all_tables)
## Clean###
#Drop unwanted variables
medals <- select(medals, NULL.V2, NULL.V3, NULL.V4, NULL.V5, NULL.V6)
#Give new Variables names
names(medals) <- c('Country', 'Gold', 'Silver', 'Bronze', 'Total')
#Convert variables class
medals$Country<-as.character(medals$Country)
for(i in 3:5) medals[, i]<-as.integer(medals[,i])
#Sort by total medals in descending order
medals <- arrange(medals, desc(Total))
View(medals)
library(httr)
library(dplyr)
library(XML)
URL <- 'http://www.bbc.com/sport/winter-olympics/2014/medals/countries'
# Get and parse all tables on the webpage
all_tables <- URL %>% GET() %>%
content(as = 'parsed') %>%
readHTMLTable()
names(all_tables) #Table has no ID
##Convert to a data frame
medals <- as.data.frame(all_tables)
## Clean###
#Drop unwanted variables
medals <- select(medals, -NULL.V1, -NULL.V7)
#Give new Variables names
names(medals) <- c('Country', 'Gold', 'Silver', 'Bronze', 'Total')
#Convert variables class
medals$Country<-as.character(medals$Country)
for(i in 3:5) medals[, i]<-as.integer(medals[,i])
#Sort by total medals in descending order
medals <- arrange(medals, desc(Total))
View(medals)
View(medals)
save.image("~/HSoG/DataAnalysis/GitHub/class/environmentexclass7.RData")
getwd
install.packages("spgwr")
########################
# Lars Mehwald and Daniel Salgado Moreno
# 13 November 2015
# Assignment 3
# German Elections 2013
########################
# Loading required packages
Packages <- c("rio", "dplyr", "tidyr", "repmis", "httr", "knitr", "ggplot2",
"xtable", "stargazer", "texreg", "lmtest", "sandwich", "Zelig",
"ggmap", "rworldmap")
lapply(Packages, require, character.only = TRUE)
# Setting the commonly used working directory
possible_dir <- c('D:/Eigene Dokumente/!1 Vorlesungen/!! WS 2015/Introduction to Collaborative Social Science Data Analysis/Assignment3',
'~/HSoG/DataAnalysis/GitHub/Assignment3')
set_valid_wd(possible_dir)
rm(possible_dir)
# setwd("D:/Eigene Dokumente/!1 Vorlesungen/!! WS 2015/Introduction to Collaborative Social Science Data Analysis/Assignment3")
# Citing R packages
LoadandCite(Packages, file = 'References/RpackageCitations.bib')
rm(Packages)
install.packages('ggthemes', dependencies = TRUE)
########################
# Lars Mehwald and Daniel Salgado Moreno
# 11 December 2015
# Master file
########################
# Setting the commonly used working directory
library("repmis")
possible_dir <- c("D:/Eigene Dokumente/!1 Vorlesungen/!! WS 2015/Introduction to Collaborative Social Science Data Analysis/Assignment3",
"~/HSoG/DataAnalysis/GitHub/Assignment3")
set_valid_wd(possible_dir)
rm(possible_dir)
# Loading required packages
source("Analysis/RPackages.R")
# Citation of packages
source("References/CiteRPackages.R")
# Loading the data set
source("Analysis/DataMerging.R")
########################
# Geo codes and maps
########################
# Loading Shape files downloaded from GADM database (www.gadm.org): by country: Germany (shapefile)
spdf <- readOGR(dsn="Analysis/data/DEU_adm_shp", layer= "DEU_adm2") # loading layer for districts
# following guidlines in: https://www.nceas.ucsb.edu/~frazier/RSpatialGuides/OverviewCoordinateReferenceSystems.pdf
spdf <- spTransform(spdf, CRS("+init=epsg:4326")) # transforming layers in the map
# NOTE: The following algorithm was taken from http://mazamascience.com/WorkingWithData/?p=1277
# Identifying the attributes in spdf to keep and associate new names with them
names(spdf)
attributes <- c("ID_1", "NAME_1", "NAME_2", "CCA_2")
# User friendly names
newNames <- c("StateId", "StateName", "DistrictName", "district")
# Subset the full dataset extracting only the desired attributes
spdf_subset <- spdf[,attributes]
# Assign the new attribute names
names(spdf_subset) <- newNames
# Create a dataframe name (potentially different from shapefile Name)
data_name <- "GermanDistricts"
# Reproject the data onto a "longlat" projection and assign it to the new name
assign(data_name,spTransform(spdf_subset, CRS("+proj=longlat")))
# The GermanDistricts dataset is now projected in latitude longitude coordinates as a //
# SpatialPolygonsDataFrame. We save the converted data as .RData for faster loading in the future.
save(list=c(data_name),file=paste("Analysis/data/GermanDistricts.RData",sep="/"))
rm(data_name, attributes, newNames)
# Load again GermanDistricts # Not necessary for now
#file <- paste("Analysis/data/GermanDistricts.RData",sep="/")
#load(file)
# Inspecting the object class for district within GermanDistricts for merging purposes
class(GermanDistricts$district) #=factor
# Transformation of class needed for district ID in SpatialPolygonsDataFrame
GermanDistricts@data$district <- as.integer(as.character(GermanDistricts@data$district))
###########################
# Preparing data for matching
###########################
# user friendly name for main variable of interest
names(DistrictData)[names(DistrictData) == 'murderAndManslaughter'] <- 'Murder'
# List of relevant variables for analysis
columns <- c(1,10,17,23,25,48,51,52,53,54,55,56,58,60,62)
# Subset the DistrictData full data frame to extract only desire variables:
DistrictData_subset <- subset(DistrictData, select=columns)
# Changing district ID for Berlin and Hamburg.
DistrictData_subset[2,1]=11000
DistrictData_subset[1,1]=02000
# Delete Bodensee observation in SPDF
GermanDistricts@data <- GermanDistricts@data[-6,]
# Order both dataframes by district ID
DistrictData_subset <- DistrictData_subset[order(DistrictData_subset$district),]
GermanDistricts@data <- GermanDistricts@data[order(GermanDistricts$district),]
# Transformation of class needed for district ID in SpatialPolygonsDataFrame
DistrictData_subset$district <- as.integer(DistrictData_subset$district)
########################
# Geo codes and maps
########################
# Loading Shape files downloaded from GADM database (www.gadm.org): by country: Germany (shapefile)
spdf <- readOGR(dsn="Analysis/data/DEU_adm_shp", layer= "DEU_adm2") # loading layer for districts
# following guidlines in: https://www.nceas.ucsb.edu/~frazier/RSpatialGuides/OverviewCoordinateReferenceSystems.pdf
spdf <- spTransform(spdf, CRS("+init=epsg:4326")) # transforming layers in the map
# NOTE: The following algorithm was taken from http://mazamascience.com/WorkingWithData/?p=1277
# Identifying the attributes in spdf to keep and associate new names with them
names(spdf)
attributes <- c("ID_1", "NAME_1", "NAME_2", "CCA_2")
# User friendly names
newNames <- c("StateId", "StateName", "DistrictName", "district")
# Subset the full dataset extracting only the desired attributes
spdf_subset <- spdf[,attributes]
# Assign the new attribute names
names(spdf_subset) <- newNames
# Create a dataframe name (potentially different from shapefile Name)
data_name <- "GermanDistricts"
# Reproject the data onto a "longlat" projection and assign it to the new name
assign(data_name,spTransform(spdf_subset, CRS("+proj=longlat")))
# The GermanDistricts dataset is now projected in latitude longitude coordinates as a //
# SpatialPolygonsDataFrame. We save the converted data as .RData for faster loading in the future.
save(list=c(data_name),file=paste("Analysis/data/GermanDistricts.RData",sep="/"))
rm(data_name, attributes, newNames)
# Load again GermanDistricts # Not necessary for now
#file <- paste("Analysis/data/GermanDistricts.RData",sep="/")
#load(file)
# Inspecting the object class for district within GermanDistricts for merging purposes
class(GermanDistricts$district) #=factor
# Transformation of class needed for district ID in SpatialPolygonsDataFrame
GermanDistricts@data$district <- as.integer(as.character(GermanDistricts@data$district))
library("rgdal", lib.loc="/Library/Frameworks/R.framework/Versions/3.2/Resources/library")
########################
# Lars Mehwald and Daniel Salgado Moreno
# 11 December 2015
# Master file
########################
# Setting the commonly used working directory
library("repmis")
possible_dir <- c("D:/Eigene Dokumente/!1 Vorlesungen/!! WS 2015/Introduction to Collaborative Social Science Data Analysis/Assignment3",
"~/HSoG/DataAnalysis/GitHub/Assignment3")
set_valid_wd(possible_dir)
rm(possible_dir)
# Loading required packages
source("Analysis/RPackages.R")
# Citation of packages
source("References/CiteRPackages.R")
# Loading the data set
source("Analysis/DataMerging.R")
########################
# Geo codes and maps
########################
# Loading Shape files downloaded from GADM database (www.gadm.org): by country: Germany (shapefile)
spdf <- readOGR(dsn="Analysis/data/DEU_adm_shp", layer= "DEU_adm2") # loading layer for districts
# following guidlines in: https://www.nceas.ucsb.edu/~frazier/RSpatialGuides/OverviewCoordinateReferenceSystems.pdf
spdf <- spTransform(spdf, CRS("+init=epsg:4326")) # transforming layers in the map
# NOTE: The following algorithm was taken from http://mazamascience.com/WorkingWithData/?p=1277
# Identifying the attributes in spdf to keep and associate new names with them
names(spdf)
attributes <- c("ID_1", "NAME_1", "NAME_2", "CCA_2")
# User friendly names
newNames <- c("StateId", "StateName", "DistrictName", "district")
# Subset the full dataset extracting only the desired attributes
spdf_subset <- spdf[,attributes]
# Assign the new attribute names
names(spdf_subset) <- newNames
# Create a dataframe name (potentially different from shapefile Name)
data_name <- "GermanDistricts"
# Reproject the data onto a "longlat" projection and assign it to the new name
assign(data_name,spTransform(spdf_subset, CRS("+proj=longlat")))
# The GermanDistricts dataset is now projected in latitude longitude coordinates as a //
# SpatialPolygonsDataFrame. We save the converted data as .RData for faster loading in the future.
save(list=c(data_name),file=paste("Analysis/data/GermanDistricts.RData",sep="/"))
rm(data_name, attributes, newNames)
# Load again GermanDistricts # Not necessary for now
#file <- paste("Analysis/data/GermanDistricts.RData",sep="/")
#load(file)
# Inspecting the object class for district within GermanDistricts for merging purposes
class(GermanDistricts$district) #=factor
# Transformation of class needed for district ID in SpatialPolygonsDataFrame
GermanDistricts@data$district <- as.integer(as.character(GermanDistricts@data$district))
###########################
# Preparing data for matching
###########################
# user friendly name for main variable of interest
names(DistrictData)[names(DistrictData) == 'murderAndManslaughter'] <- 'Murder'
# List of relevant variables for analysis
columns <- c(1,10,17,23,25,48,51,52,53,54,55,56,58,60,62)
# Subset the DistrictData full data frame to extract only desire variables:
DistrictData_subset <- subset(DistrictData, select=columns)
# Changing district ID for Berlin and Hamburg.
DistrictData_subset[2,1]=11000
DistrictData_subset[1,1]=02000
# Delete Bodensee observation in SPDF
GermanDistricts@data <- GermanDistricts@data[-6,]
# Order both dataframes by district ID
DistrictData_subset <- DistrictData_subset[order(DistrictData_subset$district),]
GermanDistricts@data <- GermanDistricts@data[order(GermanDistricts$district),]
# Transformation of class needed for district ID in SpatialPolygonsDataFrame
DistrictData_subset$district <- as.integer(DistrictData_subset$district)
# Merge DistrictData and GermanDistricts by "district"
GermanDistricts@data <- left_join(GermanDistricts@data, DistrictData_subset, by="district")
#########################
#Plotting Murder Rate per district
#########################
# Cut data into classes
classes <- cut(DistrictData_subset$MurderRate, c(0,1,2,3,4,5,10,15,29.270), right = FALSE)
levels(classes) <- c("0", "1", "2", "3","4", "5", "10", "15 or higher")
# Assign colors
colours <- brewer.pal(8,"Reds") # Pick color palette
# Plot the shapefiles colored
plot(GermanDistricts,border = "darkgrey", col = colours[classes])
View(DistrictData)
View(DistrictData)
names(DistrictData)
########################
# Lars Mehwald and Daniel Salgado Moreno
# 11 December 2015
# Master file
########################
# Setting the commonly used working directory
library("repmis")
possible_dir <- c("D:/Eigene Dokumente/!1 Vorlesungen/!! WS 2015/Introduction to Collaborative Social Science Data Analysis/Assignment3",
"~/HSoG/DataAnalysis/GitHub/Assignment3")
set_valid_wd(possible_dir)
rm(possible_dir)
# Loading required packages
source("Analysis/RPackages.R")
# Citation of packages
source("References/CiteRPackages.R")
# Loading the data set
source("Analysis/DataMerging.R")
View(DistrictData_subset)
########################
# Lars Mehwald and Daniel Salgado Moreno
# 11 December 2015
# Master file
########################
# Setting the commonly used working directory
library("repmis")
possible_dir <- c("D:/Eigene Dokumente/!1 Vorlesungen/!! WS 2015/Introduction to Collaborative Social Science Data Analysis/Assignment3",
"~/HSoG/DataAnalysis/GitHub/Assignment3")
set_valid_wd(possible_dir)
rm(possible_dir)
# Loading required packages
source("Analysis/RPackages.R")
# Citation of packages
source("References/CiteRPackages.R")
# Loading the data set
source("Analysis/DataMerging.R")
# Creating a map
# source("Analysis/GeoCodesMaps.R")
# Performing the analyses
source("Analysis/DataAnalysis.R")
# creating graphs
# source("Analysis/Graphs.R")
# Remove everything
# rm(list=ls())
# Murder Histogram
histMurder <- ggplot(DistrictData, aes(murderAndManslaughter)) +
geom_histogram(binwidth=1, colour="black", fill="red") +
xlab("Murders") +
ylab("Counts per district") +
ggtitle("District count for Murder count: right skewed")
plot(histMurder)
# Murder Histogram
histMurder <- ggplot(DistrictData, aes(Murder)) +
geom_histogram(binwidth=1, colour="black", fill="red") +
xlab("Murders") +
ylab("Counts per district") +
ggtitle("District count for Murder count: right skewed")
plot(histMurder)
# Murder Rate Histogram
histMurderRate <- ggplot(DistrictData, aes(MurderRate)) +
geom_histogram(binwidth=1, colour="black", fill="red") +
xlab("Murder Rate per district") +
ylab("District count") +
ggtitle("District count for Murder Rates: right skewed")
plot(histMurderRate)
# Murder Histogram
histMurder <- ggplot(DistrictData, aes(Murder)) +
geom_histogram(binwidth=2, colour="black", fill="red") +
xlab("Murders") +
ylab("Counts per district") +
ggtitle("District count for Murder count: right skewed")
plot(histMurder)
# Murder Histogram
histMurder <- ggplot(DistrictData, aes(Murder)) +
geom_histogram(binwidth=1, colour="black", fill="red") +
xlab("Murders") +
ylab("Counts per district") +
ggtitle("District count for Murder count: right skewed")
plot(histMurder)
summary(DistrictData$Murder)
